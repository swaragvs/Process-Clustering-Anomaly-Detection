# -*- coding: utf-8 -*-
"""Process-Resource Clustering K Means.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1djQ7_abzsYdS_8tiJjChuDc0r-opvTpy
"""

# -----------------------------------------------------------------
# Cell 1: Setup - Imports, Configuration, and Utilities
# -----------------------------------------------------------------

# --- Core Libraries ---
import os
import sys
import warnings
import numpy as np
import pandas as pd
import joblib

# --- Plotting Libraries ---
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go

# --- Scikit-learn (sklearn) Modules ---
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.ensemble import IsolationForest
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score

# ----------------------------- CONFIGURATION -----------------------------

# File and Directory Paths
DATA_PATH = 'process_log.csv'
SAVE_DIR = 'models'

# Model & Plotting Constants
RANDOM_STATE = 42
PLOTLY_THEME = 'plotly_dark'

# Define standard color palettes
COLOR_NORMAL = '#4da6ff' # Blue for normal
COLOR_ABNORMAL = '#ff4d4d' # Red for abnormal
COLOR_CLUSTER_PALETTE = ['#ff4d4d', '#4da6ff', '#ffa64d', '#66cc66', '#b366ff', '#ff66b3']

# ----------------------------- UTILITIES -----------------------------

def print_sep(title=''):
    """Prints a formatted section separator with a title."""
    print('\n' + '-'*60)
    if title:
        print(f"--- {title.upper()} ---")
    print('-'*60 + '\n')

def ensure_dir(path):
    """Checks if a directory exists, and if not, creates it."""
    if not os.path.exists(path):
        print(f"Creating directory: {path}")
        os.makedirs(path)
    else:
        print(f"Directory already exists: {path}")

# ----------------------------- INITIALIZATION -----------------------------

# Set global random seed for reproducibility
np.random.seed(RANDOM_STATE)

# Set global plotting themes
plt.style.use('dark_background')
px.defaults.template = PLOTLY_THEME

# Suppress common warnings for cleaner output
warnings.filterwarnings('ignore')

# Run utility to create the models directory
ensure_dir(SAVE_DIR)

print("Cell 1 executed: All libraries imported, configuration set, and utilities defined.")

# -----------------------------------------------------------------
# Cell 2: Data Loading and Initial Inspection
# -----------------------------------------------------------------

print_sep("DATA LOADING AND INITIAL INSPECTION")

try:
    # Load the CSV file
    df = pd.read_csv(DATA_PATH)

    # --- Initial Inspection ---
    print(f"Loaded data from: '{DATA_PATH}'")
    print(f"Dataset dimensions (shape): {df.shape}")
    print("\nColumn list:")
    print(list(df.columns))

    print("\nFirst 5 rows (head):")
    display(df.head())

    # --- Define Feature Schema ---
    features = [
        "cpu_percent",
        "memory_percent",
        "disk_read_mb",
        "disk_write_mb",
        "net_sent_mb",
        "net_recv_mb"
    ]

    # --- Schema Validation ---
    print("\nValidating feature schema...")
    missing_cols = [col for col in features if col not in df.columns]

    if missing_cols:
        print(f"ERROR: The following required columns are missing: {missing_cols}")
        raise ValueError("Missing required feature columns in the dataset.")
    else:
        print("All required feature columns are present.")

    # --- Optional: Data Types and Summary Stats ---
    print("\nData Types (info):")
    df.info()

    print("\nSummary Statistics (describe) for features:")
    display(df[features].describe().T)

    # Keep a copy of the raw features for later EDA
    df_raw = df[features].copy()

    print("\nCell 2 executed: Data loaded and schema verified.")

except FileNotFoundError:
    print(f"FATAL ERROR: Data file not found at '{DATA_PATH}'")
    print("Please ensure the file is in the correct directory.")
except Exception as e:
    print(f"An error occurred during data loading: {e}")

# -----------------------------------------------------------------
# Cell 3: EDA - Feature Distributions
# -----------------------------------------------------------------

print_sep("EDA: FEATURE DISTRIBUTIONS")

# Note: plt.style.use('dark_background') was set in Cell 1.

# Define the number of features
num_features = len(features)

# Create a subplot grid: 1 row for each feature, 2 columns (hist + box)
fig, axes = plt.subplots(
    nrows=num_features,
    ncols=2,
    figsize=(14, 4 * num_features),  # 14 wide, 4 high per feature
    facecolor='k'                   # Ensure figure background is black
)

print(f"Generating {num_features} distribution plots (Histogram + Boxplot)...")

for i, col in enumerate(features):

    # Get the specific axes for this feature's row
    ax_hist = axes[i, 0]
    ax_box = axes[i, 1]

    # Get the data, dropping NaNs for plotting
    data_to_plot = df_raw[col].dropna()

    # --- 1. Histogram ---
    # Use a color from our palette
    plot_color = COLOR_CLUSTER_PALETTE[i % len(COLOR_CLUSTER_PALETTE)]

    ax_hist.hist(data_to_plot, bins=50, color=plot_color, alpha=0.8)
    ax_hist.set_title(f'Distribution: {col}')
    ax_hist.set_ylabel('Frequency')
    ax_hist.grid(False) # Clean grid

    # --- 2. Boxplot ---
    # A horizontal boxplot is often easier to read side-by-side
    boxplot_props = dict(
                         medianprops=dict(color='white', linewidth=2),
                         boxprops=dict(facecolor=plot_color, color=plot_color, alpha=0.8),
                         whiskerprops=dict(color=plot_color, linestyle='--'),
                         capprops=dict(color=plot_color))

    ax_box.boxplot(data_to_plot, vert=False, patch_artist=True, **boxplot_props)
    ax_box.set_title(f'Boxplot: {col}')
    ax_box.set_yticks([]) # Remove y-axis ticks for a cleaner look
    ax_box.grid(False) # Clean grid

# Adjust layout to prevent labels from overlapping
plt.tight_layout()
plt.show()

print("\nCell 3 executed: Feature distributions and outlier profiles visualized.")

# -----------------------------------------------------------------
# Cell 4: EDA - Correlation Analysis
# -----------------------------------------------------------------

print_sep("EDA: CORRELATION ANALYSIS")

# Compute the correlation matrix for the raw features
corr_matrix = df_raw.corr()

print("Correlation Matrix:")
# Display the matrix, rounded for readability
display(corr_matrix.round(2))

# --- Plot the Heatmap ---
fig, ax = plt.subplots(
    figsize=(10, 8),  # Create a slightly larger figure for clarity
    facecolor='k'
)

# Create the heatmap using imshow
# cmap='coolwarm' is a good choice for correlation (blue=-1, red=1)
im = ax.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)

# --- Add Colorbar ---
cbar = plt.colorbar(im, ax=ax, shrink=0.8) # Add a color bar
cbar.set_label('Correlation Coefficient', rotation=-90, va="bottom")

# --- Set Ticks and Labels ---
# We want ticks to be in the center of each cell
ax.set_xticks(np.arange(len(features)))
ax.set_yticks(np.arange(len(features)))

# Label ticks with the feature names
ax.set_xticklabels(features, rotation=45, ha='right')
ax.set_yticklabels(features)

ax.set_title('Feature Correlation Heatmap', fontsize=16)

# --- Annotate Heatmap with Values ---
# Loop over data dimensions and create text annotations.
for i in range(len(features)):
    for j in range(len(features)):
        # Determine text color based on background luminance
        val = corr_matrix.iloc[i, j]
        color = 'white' if abs(val) > 0.6 else 'black'

        ax.text(
            j, i,
            f'{val:.2f}', # Format to 2 decimal places
            ha="center",
            va="center",
            color=color,
            fontsize=10
        )

plt.tight_layout() # Adjust layout to prevent labels from overlapping
plt.show()

print("\nCell 4 executed: Correlation matrix computed and visualized.")

# -----------------------------------------------------------------
# Cell 5: EDA - Temporal Behavior (Aggregated)
# -----------------------------------------------------------------

print_sep("EDA: TEMPORAL BEHAVIOR (SYSTEM-WIDE TOTALS)")

# --- Check for a timestamp column ---
time_col = None
if 'timestamp' in df.columns:
    time_col = 'timestamp'
elif 'time' in df.columns:
    time_col = 'time'
elif 'datetime' in df.columns:
    time_col = 'datetime'

# --- If column exists, aggregate and plot. ---
if time_col:
    print(f"Timestamp column ('{time_col}') found. Aggregating and plotting time-series...")

    # *** NEW: Import for date formatting ***
    import matplotlib.dates as mdates
    # We also need pandas for Timedelta
    import pandas as pd

    try:
        # -----------------------------------------------------------------
        # --- AGGREGATION LOGIC ---
        # -----------------------------------------------------------------
        print("Aggregating per-process data into system-wide totals...")

        df_time_series = df.groupby(time_col).agg(
            total_cpu_percent=('cpu_percent', 'sum'),
            total_memory_percent=('memory_percent', 'sum'),
            total_disk_read_mb=('disk_read_mb', 'sum'),
            total_disk_write_mb=('disk_write_mb', 'sum'),
            total_net_sent_mb=('net_sent_mb', 'first'),
            total_net_recv_mb=('net_recv_mb', 'first')
        ).reset_index()

        plot_features = [col for col in df_time_series.columns if col != time_col]
        num_features = len(plot_features)

        df_time_series[time_col] = pd.to_datetime(df_time_series[time_col], unit='s', errors='coerce')
        df_time_series = df_time_series.dropna(subset=[time_col])

        print(f"Aggregated data has {len(df_time_series)} timestamps.")

        # -----------------------------------------------------------------
        # --- PLOTTING ---
        # -----------------------------------------------------------------

        fig, axes = plt.subplots(
            nrows=num_features,
            ncols=1,
            figsize=(16, 3 * num_features),
            sharex=True,
            facecolor='k'
        )

        fig.suptitle('Time-Series Analysis of Total System Features', fontsize=18, y=1.02)

        for i, col in enumerate(plot_features):
            ax = axes[i] if num_features > 1 else axes
            plot_color = COLOR_CLUSTER_PALETTE[i % len(COLOR_CLUSTER_PALETTE)]

            ax.plot(
                df_time_series[time_col],
                df_time_series[col],
                color=plot_color,
                linewidth=1,
                alpha=0.9
            )

            ax.set_ylabel(col, fontsize=12)
            ax.grid(True, linestyle='--', alpha=0.3)
            ax.set_title(f'{col} over Time', loc='left')

        # --- Get the bottom axis for formatting ---
        ax_bottom = axes[num_features - 1] if num_features > 1 else axes
        ax_bottom.set_xlabel('Timestamp', fontsize=12)

        # --- Use AutoDateLocator for smart ticking ---
        ax_bottom.xaxis.set_major_locator(mdates.AutoDateLocator(minticks=5, maxticks=10))
        ax_bottom.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S.%f'))


        # -----------------------------------------------------------------
        # --- NEW: CODE FOR ZOOMING (TUNING) ---
        # -----------------------------------------------------------------

        # 1. Get the full time range
        min_time = df_time_series[time_col].min()
        max_time = df_time_series[time_col].max()

        # 2. Define your desired view window
        # --- TUNE THESE LINES to expand or contract ---

        # # Option A: Show the first 5 seconds
        # start_time = min_time
        # end_time = start_time + pd.Timedelta(seconds=1)

        # Option B: Show the first 500 milliseconds (zoomed in)
        # start_time = min_time
        # end_time = start_time + pd.Timedelta(milliseconds=500)

        # Option C: Show the full plot (no zoom)
        start_time, end_time = min_time, max_time

        # 3. Apply the limits to the x-axis
        print(f"Setting x-axis view from {start_time} to {end_time}")
        ax_bottom.set_xlim(start_time, end_time)

        # --- END OF NEW CODE ---


        plt.tight_layout()

        # (Optional) Rotate labels for better readability
        plt.xticks(rotation=30, ha='right')

        plt.show()

    except Exception as e:
        print(f"Error during time-series plotting: {e}")
        print("Skipping time-series plot.")

else:
    print("No 'timestamp', 'time', or 'datetime' column detected.")
    print("Skipping time-series EDA.")

print("\nCell 5 executed.")

# -----------------------------------------------------------------
# Cell 6: Preprocessing - Missing Values & Outlier Removal
# -----------------------------------------------------------------

print_sep("PREPROCESSING: MISSING VALUES & OUTLIER REMOVAL")

if 'df_raw' not in locals():
    print("ERROR: 'df_raw' not found. Please re-run Cell 2.")
else:
    initial_rows = len(df_raw)
    print(f"Initial raw data rows: {initial_rows}")

    # --- 1. Handle Missing Values (Imputation) ---
    print("Checking for missing values...")
    missing_counts = df_raw.isna().sum()

    if missing_counts.sum() > 0:
        print(f"Found {missing_counts.sum()} missing values. Imputing with median...")
        print(missing_counts[missing_counts > 0])

        # Impute missing values using the median of each feature
        df_imputed = df_raw.fillna(df_raw.median())
    else:
        print("No missing values found.")
        df_imputed = df_raw.copy()

    # --- 2. Handle Outliers (SKIPPED as per user logic) ---
    print("\nSkipping outlier removal step as per logic.")
    # The imputed dataframe is now our "clean" dataframe
    df_clean = df_imputed

    # --- 3. Print Before-After Report ---
    cleaned_rows = len(df_clean)
    rows_removed = initial_rows - cleaned_rows

    # Handle division by zero if initial_rows is 0
    percent_removed = 0.0
    if initial_rows > 0:
        percent_removed = (rows_removed / initial_rows) * 100

    print("\n--- Preprocessing Summary ---")
    print(f"Original rows:       {initial_rows}")
    print(f"Rows after imputation: {df_imputed.shape[0]}")
    print(f"Rows after cleaning:   {cleaned_rows}")
    print("-------------------------------")
    print(f"Total rows removed:  {rows_removed} ({percent_removed:.2f}%)")

    # (Optional) Save a snapshot for auditing
    # output_csv_path = os.path.join(SAVE_DIR, 'process_log_cleaned.csv')
    # df_clean.to_csv(output_csv_path, index=False)
    # print(f"Cleaned data snapshot saved to: {output_csv_path}")

    print("\nCell 6 executed: Missing values handled. Outlier removal was skipped.")

# -----------------------------------------------------------------
# Cell 7: Preprocessing - Scaling & Dimensionality Reduction
# -----------------------------------------------------------------

print_sep("SCALING & DIMENSIONALITY REDUCTION (PCA)")

if 'df_clean' not in locals():
    print("ERROR: 'df_clean' not found. Please re-run Cell 6.")
else:
    # --- 1. Apply StandardScaler ---
    print(f"Applying StandardScaler to {len(df_clean)} rows...")

    # Initialize the scaler
    scaler = StandardScaler()

    # Fit the scaler to the clean data and transform it
    X_scaled = scaler.fit_transform(df_clean[features])

    # Save the fitted scaler
    scaler_path = os.path.join(SAVE_DIR, 'scaler.joblib')
    joblib.dump(scaler, scaler_path)
    print(f"Scaler model saved to: {scaler_path}")

    # --- 2. Apply PCA (n_components=3) ---
    print("\nApplying PCA and reducing to 3 components...")

    # Initialize PCA for 3 components
    pca = PCA(n_components=3, random_state=RANDOM_STATE)

    # Fit PCA to the *scaled* data and transform it
    X_pca = pca.fit_transform(X_scaled)

    # Save the fitted PCA model
    pca_path = os.path.join(SAVE_DIR, 'pca.joblib')
    joblib.dump(pca, pca_path)
    print(f"PCA model saved to: {pca_path}")

    # --- 3. Report Explained Variance ---
    explained_variance_ratio = pca.explained_variance_ratio_
    total_explained_variance = np.sum(explained_variance_ratio) * 100

    print("\n--- PCA Explained Variance ---")
    print(f"Component 1: {explained_variance_ratio[0]*100:.2f}%")
    print(f"Component 2: {explained_variance_ratio[1]*100:.2f}%")
    print(f"Component 3: {explained_variance_ratio[2]*100:.2f}%")
    print(f"Total Explained: {total_explained_variance:.2f}%")

    print(f"\nInterpretation: The 3 PCA components successfully captured "
          f"{total_explained_variance:.2f}% of the total information (variance) "
          f"from the original 6 features. This ensures our 3D plots will be a "
          f"meaningful and accurate representation of the data's structure.")

    print("\nCell 7 executed: Data scaled, PCA applied, and models saved.")

# -----------------------------------------------------------------
# Cell 8: K-Means Model Selection - Elbow & Silhouette Analysis
# -----------------------------------------------------------------

print_sep("K-MEANS MODEL SELECTION: ELBOW & SILHOUETTE ANALYSIS")

if 'X_scaled' not in locals():
    print("ERROR: 'X_scaled' data not found. Please re-run Cell 7.")
else:
    # Define the range of k (number of clusters) to test
    k_range = range(2, 11)

    # Lists to store the metrics
    sse_scores = []
    silhouette_scores = []

    print(f"Calculating SSE and Silhouette scores for k from {min(k_range)} to {max(k_range)}...")
    print("This may take a moment...")

    # Loop through each value of k
    for k in k_range:
        print(f"  > Processing k = {k}...")

        # Initialize and train a temporary KMeans model
        kmeans_temp = KMeans(
            n_clusters=k,
            n_init=10,  # Run 10 times with different seeds
            random_state=RANDOM_STATE
        )
        kmeans_temp.fit(X_scaled)

        # 1. Record SSE (Sum of Squared Errors)
        # 'inertia_' attribute in KMeans is the SSE
        sse_scores.append(kmeans_temp.inertia_)

        # 2. Record Silhouette Score
        # This score measures how similar an object is to its own cluster
        # compared to other clusters. Score is from -1 to 1 (higher is better).
        labels = kmeans_temp.labels_
        sil_score = silhouette_score(X_scaled, labels, random_state=RANDOM_STATE)
        silhouette_scores.append(sil_score)

    # --- Store results in a DataFrame for easy analysis ---
    df_scores = pd.DataFrame({
        'k': list(k_range),
        'SSE (Elbow)': sse_scores,
        'Silhouette Score': silhouette_scores
    })

    print("\n--- Model Selection Metrics ---")
    display(df_scores.set_index('k').round(3))

    print("\nCell 8 executed: SSE and Silhouette scores calculated.")

# -----------------------------------------------------------------
# Cell 9: K-Means Model Selection - Visualization
# -----------------------------------------------------------------

print_sep("K-MEANS MODEL SELECTION: VISUALIZATION")

if 'df_scores' not in locals():
    print("ERROR: 'df_scores' not found. Please re-run Cell 8.")
else:
    # --- 1. Find the best_k (based on max Silhouette Score) ---
    best_k = df_scores.loc[df_scores['Silhouette Score'].idxmax()]['k']

    print(f"Optimal 'k' (max Silhouette Score): {best_k}")

    # --- 2. Create the 2-panel figure ---
    fig, axes = plt.subplots(1, 2, figsize=(16, 6), facecolor='k')

    # --- Panel 1: Elbow Method (SSE) ---
    ax1 = axes[0]
    ax1.plot(
        df_scores['k'],
        df_scores['SSE (Elbow)'],
        marker='o',
        linestyle='-',
        color=COLOR_NORMAL
    )
    ax1.set_title('Elbow Method (SSE)', fontsize=16)
    ax1.set_xlabel('Number of Clusters (k)', fontsize=12)
    ax1.set_ylabel('Sum of Squared Errors (SSE)', fontsize=12)
    ax1.grid(True, linestyle='--', alpha=0.3)

    # Highlight the chosen k
    ax1.axvline(
        x=best_k,
        color=COLOR_ABNORMAL,
        linestyle='--',
        linewidth=2,
        label=f'Optimal k = {best_k}'
    )
    ax1.legend()
    ax1.set_xticks(df_scores['k']) # Ensure integer ticks for k

    # --- Panel 2: Silhouette Scores ---
    ax2 = axes[1]
    ax2.plot(
        df_scores['k'],
        df_scores['Silhouette Score'],
        marker='o',
        linestyle='-',
        color='#66cc66' # Green for a score
    )
    ax2.set_title('Silhouette Scores', fontsize=16)
    ax2.set_xlabel('Number of Clusters (k)', fontsize=12)
    ax2.set_ylabel('Average Silhouette Score', fontsize=12)
    ax2.grid(True, linestyle='--', alpha=0.3)

    # Highlight the chosen k
    ax2.axvline(
        x=best_k,
        color=COLOR_ABNORMAL,
        linestyle='--',
        linewidth=2,
        label=f'Optimal k = {best_k}'
    )
    ax2.legend()
    ax2.set_xticks(df_scores['k']) # Ensure integer ticks for k

    # --- Display the plots ---
    fig.suptitle('K-Means Model Selection Analysis', fontsize=20, y=1.05)
    plt.tight_layout()
    plt.show()

    print("\nCell 9 executed: Cluster selection plots generated.")

# -----------------------------------------------------------------
# Cell 10: K-Means Model Training & Evaluation
# -----------------------------------------------------------------

print_sep("K-MEANS MODEL TRAINING & EVALUATION")

# Ensure necessary data is available
if 'X_scaled' not in locals() or 'df_scores' not in locals():
    print("ERROR: Missing required data (X_scaled or df_scores). Please run Cells 7, 8, and 9.")
else:
    # --- 1. Select best_k ---
    # Find the k that gave the maximum Silhouette Score
    best_k = df_scores.loc[df_scores['Silhouette Score'].idxmax()]['k']

    print(f"Finalizing K-Means model with best_k = {int(best_k)} (based on max Silhouette Score).")

    best_k = 5 # ( Overriding user defined k depending on the data nature and requirements )

    # --- 2. Train Final K-Means Model ---
    kmeans_final = KMeans(
        n_clusters=int(best_k),
        n_init=50, # Use a higher n_init for the final model
        random_state=RANDOM_STATE
    )
    kmeans_final.fit(X_scaled)
    labels = kmeans_final.labels_

    # Add cluster labels to the clean DataFrame for later use
    df_clean['cluster'] = labels

    # --- 3. Save Final Model ---
    kmeans_path = os.path.join(SAVE_DIR, 'kmeans_final.joblib')
    joblib.dump(kmeans_final, kmeans_path)
    print(f"Trained K-Means model saved to: {kmeans_path}")

    # --- 4. Compute Evaluation Metrics ---

    # Calculate all three metrics
    sil_score = silhouette_score(X_scaled, labels)
    ch_score = calinski_harabasz_score(X_scaled, labels)
    db_score = davies_bouldin_score(X_scaled, labels)

    print("\n--- Clustering Performance Metrics ---")

    # Print and interpret Silhouette Score
    print(f"1. Silhouette Score: {sil_score:.4f}")
    if sil_score > 0.5:
        print(f"   Interpretation: **GOOD SEPARATION**. A score > 0.5 indicates that clusters are dense, well-separated, and distinct, strongly supporting the validity of our {int(best_k)} behavioral profiles.")
    else:
        print(f"   Interpretation: Borderline score. Might need re-evaluation of feature scaling or cluster number.")

    print("-" * 25)

    # Print and interpret Calinski-Harabasz Index
    print(f"2. Calinski-Harabasz Index: {ch_score:.2f}")
    print("   Interpretation: **HIGHER IS BETTER**. This metric measures the ratio of between-cluster variance (separation) to within-cluster variance (density). The high value suggests the clusters are internally compact and externally well-spaced.")

    print("-" * 25)

    # Print and interpret Davies-Bouldin Index
    print(f"3. Davies-Bouldin Index: {db_score:.4f}")
    print("   Interpretation: **LOWER IS BETTER** (0.0 is perfect). This score measures the average similarity between clusters. A low value confirms the cluster shapes are complementary and distinct, with minimal overlap.")

    print("\nCell 10 executed: Final K-Means model trained and validated.")